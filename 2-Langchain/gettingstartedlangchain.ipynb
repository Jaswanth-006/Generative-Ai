{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f45e212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d9d55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32611600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cc36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a05ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said, \"Hi my name is jaswanth.\" I need to respond appropriately. First, I should acknowledge their greeting and their name. Let me start with a friendly hello. Maybe say something like, \"Hello Jaswanth!\" to make it personal.\\n\\nThen, I should ask how I can assist them today. Since they didn\\'t mention any specific problem yet, keeping it open-ended is good. Maybe add a smiley to keep the tone friendly. Let me check the structure again: greet them by name, offer assistance, and keep it warm. Yeah, that should work. Alright, time to put it all together.\\n</think>\\n\\nHello Jaswanth! ðŸ˜Š How can I assist you today? Feel free to ask me anything!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 18, 'total_tokens': 174, 'completion_time': 0.379664904, 'prompt_time': 0.003152208, 'queue_time': 0.623657768, 'total_time': 0.382817112}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None}, id='run--cdd1ec89-0583-4c84-be2a-e8a75cf2c0d7-0', usage_metadata={'input_tokens': 18, 'output_tokens': 156, 'total_tokens': 174})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "\n",
    "model.invoke(\"Hi my name is jaswanth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ea553e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hi my name is jaswanth.\" I need to respond appropriately. First, I should greet them back and mention their name to make it personal. Maybe ask how I can assist them today. Keep it friendly and open-ended so they feel comfortable to ask for help. Let me check if there\\'s any specific information I should address. Since they just introduced themselves, probably just a standard greeting. Make sure the tone is positive and welcoming. Alright, that should work.\\n\\n</think>\\n\\nHello Jaswanth! Nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 18, 'total_tokens': 160, 'completion_time': 0.350643188, 'prompt_time': 0.003411639, 'queue_time': 0.2427927, 'total_time': 0.354054827}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_98b01f25b2', 'finish_reason': 'stop', 'logprobs': None}, id='run--9b9cbf2e-ebf8-459d-acc4-7c34b5cd1b30-0', usage_metadata={'input_tokens': 18, 'output_tokens': 142, 'total_tokens': 160})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model1 = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "model.invoke(\"Hi my name is jaswanth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf13951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert AI enginner.provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## promt enginnering \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are an expert AI enginner.provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45538342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert AI enginner.provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000182EBBDA490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000182EBBDAE90>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##chaining\n",
    "\n",
    "chain = prompt|model1\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc91c660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I can definitely tell you about LangSmith! \n",
      "\n",
      "LangSmith is an open-source platform developed by the amazing team at **AI21 Labs**.  Think of it as a powerful toolset designed to help developers and researchers build and fine-tune their own large language models (LLMs).\n",
      "\n",
      "Here are some key things to know about LangSmith:\n",
      "\n",
      "**1. User-Friendly Interface:** \n",
      "\n",
      "LangSmith stands out because it offers a web-based interface that makes working with LLMs more accessible. You don't need to be a deep learning expert to get started â€“ it's designed to be intuitive and easy to use.\n",
      "\n",
      "**2. Comprehensive Toolkit:**\n",
      "\n",
      "LangSmith provides a suite of tools for the entire LLM lifecycle:\n",
      "\n",
      "* **Data Preparation:**  Tools to clean, format, and prepare your training data.\n",
      "* **Model Training:**  Capabilities to train your own LLMs using various architectures (like GPT-like models).\n",
      "* **Fine-Tuning:**  Methods to specialize pre-trained LLMs for specific tasks, like text summarization, question answering, or code generation.\n",
      "* **Evaluation:**  Metrics and techniques to assess the performance of your models.\n",
      "\n",
      "**3. Open and Collaborative:**\n",
      "\n",
      "LangSmith is built on open-source principles. This means:\n",
      "\n",
      "* **Transparency:** The code and documentation are publicly available, allowing for scrutiny and community improvement.\n",
      "* **Flexibility:** You can modify and extend LangSmith to suit your specific needs.\n",
      "* **Community:**  You can connect with other developers and researchers, share your work, and learn from others.\n",
      "\n",
      "**4. Integrations:** LangSmith integrates with popular tools and platforms, making it easy to incorporate your LLMs into existing workflows.\n",
      "\n",
      "**Where to Learn More:**\n",
      "\n",
      "* **Official LangSmith Website:** [https://www.langsmith.ai/](https://www.langsmith.ai/)\n",
      "* **GitHub Repository:** [https://github.com/AI21-Co/langsmith](https://github.com/AI21-Co/langsmith)\n",
      "\n",
      "Let me know if you have any other questions about LangSmith or LLMs in general. Happy to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\":\"can you tell me something about langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa5fde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, the user is asking about LangSmith. Let me start by recalling what I know about it. LangSmith is a tool related to LLM (Large Language Model) development, right? I remember that it's part of the LangChain ecosystem. Wait, LangChain is a framework for building applications with LLMs. So, LangSmith must be a component that supports the development and monitoring of those applications.\n",
      "\n",
      "First, I should explain what LangSmith is. It's a platform for monitoring and evaluating LLM applications. Maybe it helps in tracking the performance of models, keeping logs of interactions, and providing analytics. But I need to be more specific. I think it offers features like logging prompts and responses, tracking metrics, and visualizing data. \n",
      "\n",
      "Hmm, the user might be looking for key features, use cases, or how it integrates with other tools. They might be a developer or data scientist working with LLMs. Let me structure my answer with an overview, key features, use cases, and maybe how it fits into the LangChain ecosystem. \n",
      "\n",
      "Wait, I should also confirm if LangSmith is specifically for LangChain projects or if it's a standalone tool. Since it's part of the LangChain family, it's designed to work with LangChain but might have broader applicability. \n",
      "\n",
      "I need to mention the main components: logging of LLM interactions, tracking of inputs/outputs, performance metrics, and maybe a UI for monitoring. Also, version control for models, which is important for iterative development. \n",
      "\n",
      "What about the benefits? It helps in debugging, optimizing models, ensuring consistency, and auditing. Maybe compliance with regulations by keeping logs. \n",
      "\n",
      "Use cases could include tracking model performance over time, comparing different model versions, or analyzing user interactions. Examples like chatbots or document analysis applications. \n",
      "\n",
      "Wait, is there anything about deployment? Or is it more about development lifecycle? I think it's more for the development and testing phases, monitoring the models as they're being used. \n",
      "\n",
      "I should also mention that it's open-source, part of LangChain, so it's free to use. Maybe provide a link to the documentation or GitHub. \n",
      "\n",
      "But I should be careful not to include incorrect details. Let me make sure I'm not mixing it up with other tools. LangSmith's main focus is on observability and monitoring, so the key points are logging, tracking, metrics, and analysis. \n",
      "\n",
      "Alright, putting it all together in a structured way. Start with a definition, then key features, use cases, benefits, and integration. Make sure to highlight why someone would use itâ€”mainly to manage and improve their LLM applications effectively.\n",
      "</think>\n",
      "\n",
      "**LangSmith** is a cutting-edge platform designed to monitor, evaluate, and improve Large Language Model (LLM) applications. It is part of the **LangChain ecosystem**, a collection of tools and frameworks for building and deploying LLM-based applications. LangSmith specifically focuses on **observability** and **performance optimization** for LLM workflows. Below is a detailed breakdown:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features of LangSmith**\n",
      "1. **LLM Interaction Logging**:\n",
      "   - Tracks and records all inputs, outputs, and intermediate steps of LLM interactions (e.g., prompts, model responses, embeddings).\n",
      "   - Stores these interactions in a queryable database for later analysis.\n",
      "\n",
      "2. **Performance Metrics & Analysis**:\n",
      "   - Provides metrics like response time, cost (e.g., tokens spent), and success/failure rates.\n",
      "   - Visualizes metrics over time to identify trends, bottlenecks, or degradation in model performance.\n",
      "\n",
      "3. **Prompt & Model Versioning**:\n",
      "   - Allows tracking of different versions of prompts, models, and chains (sequences of LLM operations).\n",
      "   - Compares performance across versions to evaluate improvements or regressions.\n",
      "\n",
      "4. **Error Debugging**:\n",
      "   - Identifies and categorizes errors (e.g., model hallucinations, API failures) to help developers debug issues.\n",
      "   - Highlights problematic inputs or edge cases.\n",
      "\n",
      "5. **User Interface**:\n",
      "   - A web-based dashboard for exploring logged interactions, filtering by metadata (e.g., user ID, model version), and viewing detailed logs.\n",
      "\n",
      "6. **Cost Management**:\n",
      "   - Monitors and optimizes costs associated with LLM usage, helping teams stay within budget.\n",
      "\n",
      "7. **Integration with LangChain**:\n",
      "   - Seamlessly integrates with **LangChain** workflows, allowing developers to log and analyze data from their existing applications.\n",
      "\n",
      "---\n",
      "\n",
      "### **Use Cases**\n",
      "- **Model Monitoring**: Track how LLMs behave in production (e.g., accuracy, latency).\n",
      "- **A/B Testing**: Compare different model versions or prompt strategies.\n",
      "- **Debugging**: Identify why a model failed a specific task (e.g., incorrect context, input formatting).\n",
      "- **Compliance & Auditing**: Maintain logs for regulatory requirements or internal audits.\n",
      "- **Optimization**: Reduce costs by analyzing high-cost or low-performing queries.\n",
      "\n",
      "---\n",
      "\n",
      "### **Why Use LangSmith?**\n",
      "- **Improve Model Reliability**: Fix errors by analyzing failure patterns.\n",
      "- **Reduce Costs**: Optimize prompt design and model usage.\n",
      "- **Iterate Faster**: Test and deploy new versions of prompts or models confidently.\n",
      "- **Ensure Transparency**: Maintain traceability of decisions made by LLMs in critical applications (e.g., healthcare, finance).\n",
      "\n",
      "---\n",
      "\n",
      "### **How It Works**\n",
      "1. **Integration**: Add LangSmith logging to your LangChain workflow via Python SDKs.\n",
      "2. **Data Collection**: All LLM interactions are automatically logged.\n",
      "3. **Analysis**: Use the LangSmith UI to query, filter, and visualize data.\n",
      "4. **Actionable Insights**: Use insights to refine prompts, models, or workflows.\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Workflow**\n",
      "Suppose you have a chatbot that sometimes provides incorrect answers. With LangSmith:\n",
      "- You can:\n",
      "  - Filter logs for interactions where the answer was flagged as incorrect.\n",
      "  - Analyze the corresponding prompts and model responses.\n",
      "  - Identify patterns (e.g., ambiguous user input, insufficient context).\n",
      "  - Iterate on the prompt or model to resolve the issue.\n",
      "\n",
      "---\n",
      "\n",
      "### **Getting Started**\n",
      "- **Installation**: Install via `pip` (e.g., `pip install langsmith`).\n",
      "- **Documentation**: Visit the [LangChain documentation](https://www.langchain.com) for setup and API guides.\n",
      "- **Open-Source**: Part of the open-source LangChain project (GitHub: [LangChain](https://github.com/langchain-ai)).\n",
      "\n",
      "---\n",
      "\n",
      "### **Who Should Use It?**\n",
      "- Developers and data scientists building LLM applications.\n",
      "- Teams needing to optimize costs and reliability of LLM workloads.\n",
      "- Organizations requiring auditability (e.g., legal, healthcare, finance).\n",
      "\n",
      "LangSmith is a critical tool for ensuring LLM applications are robust, cost-effective, and aligned with business needs. For deeper technical details, explore the [LangChain docs](https://www.langchain.com) or the [LangSmith documentation](https://python.langchain.com/docs/expression_language/agents/langsmith).html).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt|model|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"can you tell me about langsmith\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64eb04d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91701cc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: <think>\nOkay, the user is asking about Langsmith and wants the answer in a JSON object. Let me start by recalling what I know about Langsmith. I think it's related to LangChain, which is a framework for developing language model applications. Wait, right, Langsmith is the observability and development tool for LangChain. It helps developers monitor, debug, and optimize their LLM applications. \n\nFirst, I need to structure the JSON with the key \"Langsmith\" and a value that's an object. The value should include a \"description\" and maybe some \"features\". The description should be a brief overview. The features should list the main capabilities like workflow management, performance monitoring, etc. I should also mention that it's part of the LangChain ecosystem.\n\nWait, let me make sure I have the key points correct. Langsmith provides tools for tracking the performance of LLM applications, allows developers to inspect model outputs, and helps in debugging. It also supports A/B testing and provides analytics. Oh, right, it's used alongside LangChain to enhance the development process.\n\nI should check if there are specific terms I need to include. Maybe mention that it's designed for MLOps in LLMs. The features list should be concise but cover the main aspects. The description needs to be a clear, one-sentence summary. Let me structure it like this:\n\n\"Langsmith\": {\n  \"description\": \"Langsmith is an observability and development platform designed for building, monitoring, and optimizing applications powered by large language models (LLMs). It is part of the LangChain ecosystem and provides tools to track performance, debug workflows, and enhance the reliability of LLM-driven applications.\",\n  \"features\": [\n    \"Workflow Tracking and Visualization\",\n    \"Performance Metrics and Analytics\",\n    \"Debugging and Error Analysis\",\n    \"A/B Testing and Model Comparison\",\n    \"Integration with LangChain Framework\"\n  ]\n}\n\nHmm, maybe \"Workflow Tracking and Visualization\" could be phrased as \"Workflow Management and Visualization\" instead? Or maybe \"Workflow tracking and analysis\". I think the current list is okay. The key is to capture the main features accurately. Let me confirm the exact terms used by Langsmith. From what I recall, the features include things like tracking LLM chains, visualizing workflows, monitoring performance, debugging, and supporting experimentation. So the features listed are appropriate.\n\nYes, that should work. The JSON structure is correct, and the information is accurate. I'll present this as the answer.\n</think>\n\n{\n  \"Langsmith\": {\n    \"description\": \"Langsmith is an observability and development platform designed for building, monitoring, and optimizing applications powered by large language models (LLMs). It is part of the LangChain ecosystem and provides tools to track performance, debug workflows, and enhance the reliability of LLM-driven applications.\",\n    \"features\": [\n      \"Workflow Tracking and Visualization\",\n      \"Performance Metrics and Analytics\",\n      \"Debugging and Error Analysis\",\n      \"A/B Testing and Model Comparison\",\n      \"Integration with LangChain Framework\"\n    ]\n  }\n}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:88\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:150\u001b[39m, in \u001b[36mparse_json_markdown\u001b[39m\u001b[34m(json_string, parser)\u001b[39m\n\u001b[32m    149\u001b[39m     json_str = json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match.group(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:166\u001b[39m, in \u001b[36m_parse_json\u001b[39m\u001b[34m(json_str, parser)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:123\u001b[39m, in \u001b[36mparse_partial_json\u001b[39m\u001b[34m(s, strict)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\json\\__init__.py:359\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    358\u001b[39m     kw[\u001b[33m'\u001b[39m\u001b[33mparse_constant\u001b[39m\u001b[33m'\u001b[39m] = parse_constant\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\json\\decoder.py:344\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    342\u001b[39m \n\u001b[32m    343\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\json\\decoder.py:362\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chain=prompt|model|output_parser\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response=\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you tell me about Langsmith?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:196\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\CSE-AI\\GEN-AI\\genaiagenticai\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:91\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     90\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output=text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Invalid json output: <think>\nOkay, the user is asking about Langsmith and wants the answer in a JSON object. Let me start by recalling what I know about Langsmith. I think it's related to LangChain, which is a framework for developing language model applications. Wait, right, Langsmith is the observability and development tool for LangChain. It helps developers monitor, debug, and optimize their LLM applications. \n\nFirst, I need to structure the JSON with the key \"Langsmith\" and a value that's an object. The value should include a \"description\" and maybe some \"features\". The description should be a brief overview. The features should list the main capabilities like workflow management, performance monitoring, etc. I should also mention that it's part of the LangChain ecosystem.\n\nWait, let me make sure I have the key points correct. Langsmith provides tools for tracking the performance of LLM applications, allows developers to inspect model outputs, and helps in debugging. It also supports A/B testing and provides analytics. Oh, right, it's used alongside LangChain to enhance the development process.\n\nI should check if there are specific terms I need to include. Maybe mention that it's designed for MLOps in LLMs. The features list should be concise but cover the main aspects. The description needs to be a clear, one-sentence summary. Let me structure it like this:\n\n\"Langsmith\": {\n  \"description\": \"Langsmith is an observability and development platform designed for building, monitoring, and optimizing applications powered by large language models (LLMs). It is part of the LangChain ecosystem and provides tools to track performance, debug workflows, and enhance the reliability of LLM-driven applications.\",\n  \"features\": [\n    \"Workflow Tracking and Visualization\",\n    \"Performance Metrics and Analytics\",\n    \"Debugging and Error Analysis\",\n    \"A/B Testing and Model Comparison\",\n    \"Integration with LangChain Framework\"\n  ]\n}\n\nHmm, maybe \"Workflow Tracking and Visualization\" could be phrased as \"Workflow Management and Visualization\" instead? Or maybe \"Workflow tracking and analysis\". I think the current list is okay. The key is to capture the main features accurately. Let me confirm the exact terms used by Langsmith. From what I recall, the features include things like tracking LLM chains, visualizing workflows, monitoring performance, debugging, and supporting experimentation. So the features listed are appropriate.\n\nYes, that should work. The JSON structure is correct, and the information is accurate. I'll present this as the answer.\n</think>\n\n{\n  \"Langsmith\": {\n    \"description\": \"Langsmith is an observability and development platform designed for building, monitoring, and optimizing applications powered by large language models (LLMs). It is part of the LangChain ecosystem and provides tools to track performance, debug workflows, and enhance the reliability of LLM-driven applications.\",\n    \"features\": [\n      \"Workflow Tracking and Visualization\",\n      \"Performance Metrics and Analytics\",\n      \"Debugging and Error Analysis\",\n      \"A/B Testing and Model Comparison\",\n      \"Integration with LangChain Framework\"\n    ]\n  }\n}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36976ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
